# -*- coding: utf-8 -*-
"""hangman_api_ankush.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sMEGM2UOrx4pfF5A_o7DZJ0-PfSwceVB
"""

import json
import requests
import random
import string
import secrets
import time
import re
import collections
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report,balanced_accuracy_score


try:
    from urllib.parse import parse_qs, urlencode, urlparse
except ImportError:
    from urlparse import parse_qs, urlparse
    from urllib import urlencode

from requests.packages.urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

import pandas as pd
import numpy as np
import itertools
from tqdm import tqdm
import string
import gc
import os
import pickle
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore')

class MemoryEfficientGaussianProcessClassifier:
    """
    Memory-efficient GP classifier with automatic dataset size management
    """
    def __init__(self, kernel_type='rbf', length_scale=1.0, max_samples=5000000):
        self.alphabet = "abcdefghijklmnopqrstuvwxyz"
        self.scaler = StandardScaler()
        self.max_samples = max_samples  # Critical: limit training samples

        # Define kernel
        if kernel_type == 'rbf':
            kernel = RBF(length_scale=length_scale) + WhiteKernel(noise_level=0.1)
        elif kernel_type == 'matern':
            kernel = Matern(length_scale=length_scale, nu=1.5) + WhiteKernel(noise_level=0.1)
        else:  # combined
            kernel = (RBF(length_scale=length_scale) +
                     Matern(length_scale=length_scale, nu=2.5) +
                     WhiteKernel(noise_level=0.1))

        self.classifier = GaussianProcessClassifier(
            kernel=kernel,
            random_state=42,
            n_restarts_optimizer=1,  # Reduced for memory efficiency
            max_iter_predict=250,     # Reduced for speed
            multi_class='one_vs_rest',
            copy_X_train=False       # Critical: saves memory
        )

    def _stratified_sample(self, X, y, max_samples):
        """
        Create stratified sample maintaining class distribution
        """
        if len(X) <= max_samples:
            return X, y

        print(f"Reducing dataset from {len(X)} to {max_samples} samples using stratified sampling...")

        # Use stratified sampling to maintain class distribution
        splitter = StratifiedShuffleSplit(
            n_splits=1,
            train_size=max_samples,
            random_state=42
        )

        sample_idx, _ = next(splitter.split(X, y))
        return X[sample_idx], y[sample_idx]

    def fit(self, X, y, verbose=True):
        """Fit GP classifier with automatic memory management"""
        print(f"Training Gaussian Process classifier...")
        print(f"Original dataset size: {len(X)} samples")

        # Critical: Reduce dataset size for GP feasibility
        X_sampled, y_sampled = self._stratified_sample(X, y, self.max_samples)

        print(f"Training on {len(X_sampled)} samples after stratified sampling")
        print(f"Class distribution: {np.bincount(y_sampled)}")

        # Scale features
        X_scaled = self.scaler.fit_transform(X_sampled)

        # Train multi-class classifier
        self.classifier.fit(X_scaled, y_sampled)

        if verbose:
            try:
                lml = self.classifier.log_marginal_likelihood()
                print(f"Training completed - Log marginal likelihood: {lml:.3f}")
            except:
                print("Training completed - Log marginal likelihood not available")

    def predict_proba(self, X):
        """Predict probabilities for all 26 letters"""
        try:
            X_scaled = self.scaler.transform(X)
            proba = self.classifier.predict_proba(X_scaled)
            return proba
        except Exception as e:
            print(f"Error in prediction: {e}")
            # Return uniform distribution as fallback
            return np.ones((len(X), 26)) / 26

    def predict(self, X):
        """Get probability predictions for single sample (for compatibility)"""
        if X.ndim == 1:
            X = X.reshape(1, -1)
        return self.predict_proba(X)

    def get_uncertainty(self, X):
        """Get prediction uncertainties from Gaussian Process"""
        try:
            X_scaled = self.scaler.transform(X)
            proba = self.classifier.predict_proba(X_scaled)
            # Calculate entropy as uncertainty measure
            entropy = -np.sum(proba * np.log(proba + 1e-10), axis=1)
            return entropy
        except Exception as e:
            print(f"Error getting uncertainty: {e}")
            return np.ones(len(X)) * 0.5

    def save(self, filename):
        """Save the trained model"""
        model_data = {
            'classifier': self.classifier,
            'scaler': self.scaler,
            'alphabet': self.alphabet,
            'max_samples': self.max_samples
        }
        with open(filename, 'wb') as file:
            pickle.dump(model_data, file)
        print(f"Model saved to {filename}")

    @classmethod
    def load(cls, filename):
        """Load a trained model"""
        with open(filename, 'rb') as file:
            model_data = pickle.load(file)

        model = cls(max_samples=model_data.get('max_samples', 5000000))
        model.classifier = model_data['classifier']
        model.scaler = model_data['scaler']
        model.alphabet = model_data['alphabet']

        return model


def build_dictionary(dictionary_file_location):
    text_file = open(dictionary_file_location,"r")
    full_dictionary = text_file.read().splitlines()
    text_file.close()
    return full_dictionary

words = build_dictionary("words_250000_train.txt")
model = MemoryEfficientGaussianProcessClassifier.load('/content/gp_hangman_model_memory_efficient.pkl')

alpha = "abcdefghijklmnopqrstuvwxyz"

value =  {"a":1,"b":2,"c":3,"d":4,"e":5,"f":6,"g":7,
        "h":8,"i":9,"j":10,"k":11,"l":12,"m":13,"n":14,
        "o":15,"p":16,"q":17,"r":18,"s":19,"t":20,"u":21,
        "v":22,"w":23,"x":24,"y":25,"z":26,"_":0}

value_rev = {v: k for k, v in value.items()}

def prediction(tc):
    tc = tc.replace(' ', '')
    inp = np.full(80, -1)
    for idx, char in enumerate(tc):
        encoded_val = value[char]
        input_vector[idx] = encoded_val
        input_vector[80 - len(tc) + idx] = encoded_val

    # Make prediction
    output = model.predict(np.array([input_vector]))[0]

    # Decode top-26 predictions to form permutation and their probabilities
    permutation = ""
    probabilities = []

    for _ in range(26):
        top_index = np.argmax(output)
        permutation += alpha[top_index]
        probabilities.append(output[top_index])
        output[top_index] = -1
    return permutation, probabilities

def create_substrings(trial,guessed=[],n=6,threshold=0.1,multiple=False):
    if guessed_letters is None:
        guessed_letters = []

    candidates = []

    # Extract masked substrings around unknown characters
    for i, char in enumerate(pattern):
        if char == '_':
            left = max(i - window_size + 1, 0)
            right = min(i + window_size, len(pattern))
            context = pattern[left:i] + '.' + pattern[i+1:right]

            if allow_multiple:
                masked_context = context.replace('_', '*')
                candidates.append(masked_context)
            elif (i == 0 or pattern[i - 1] != '_') and (i == len(pattern) - 1 or pattern[i + 1] != '_'):
                candidates.append(context)

    # Break down into fixed-length substrings
    valid_substrings = []
    for context in candidates:
        for j in range(len(context) - window_size + 1):
            snippet = context[j:j + window_size]
            valid_substrings.append(snippet)

    # Filter based on number of masked characters
    if allow_multiple:
        valid_substrings = [s for s in valid_substrings if s.count('*') < 2]
    else:
        valid_substrings = [s for s in valid_substrings if '_' not in s]

    valid_substrings = [s for s in valid_substrings if len(s) == window_size]

    if not valid_substrings:
        return None, None

    probable_letters = []

    for snippet in valid_substrings:
        dot_index = snippet.index('.')
        if allow_multiple:
            snippet = snippet.replace('*', '.')

        match_letters = []

        for word in words:
            match = re.search(snippet, word)
            if match:
                letter = word[match.start() + dot_index]
                if letter not in guessed_letters:
                    match_letters.append(letter)

        if not match_letters:
            continue

        counts = Counter(match_letters)
        weighted = [[char, freq / len(match_letters), freq] for char, freq in counts.most_common()]
        probable_letters.append(weighted)

    if not probable_letters:
        return None, None

    distribution = np.zeros(26)

    for group in probable_letters:
        for char_data in group:
            index = value[char_data[0]] - 1
            distribution[index] += char_data[1]

    distribution /= distribution.sum()

    best_index = np.argmax(distribution)

    if distribution[best_index] < min_confidence:
        return None, None

    return value_rev[best_index + 1], distribution[best_index]

def guessing(word, guessed_letters, verbose=False):
    # Clean the word by removing spaces
    word_clean = word.replace(' ', '')

    aa, probs = prediction(word_clean)
    pred_prob = 0
    pred_letter = ""

    # Find the first letter from prediction that hasn't been guessed
    for i in range(len(aa)):
        if aa[i] not in guessed_letters:
            pred_letter = aa[i]
            pred_prob = probs[i]
            break

    if verbose:
        print("prediction:", pred_letter)

    [a, prob] = create_substrings(word_clean, guessed_letters, 8)
    if a != None and prob > pred_prob:
        if verbose:
            print("used8", a)
        return a

    [a, prob] = create_substrings(word_clean, guessed_letters, 7, multiple=True)
    if a != None and prob > pred_prob:
        if verbose:
            print("used7", a)
        return a

    [a, prob] = create_substrings(word_clean, guessed_letters, 6, multiple=True)
    if a != None and prob > pred_prob:
        if verbose:
            print("used6", a)
        return a

    [a, prob] = create_substrings(word_clean, guessed_letters, 5, multiple=True)
    if a != None and prob > pred_prob:
        if verbose:
            print("used5", a)
        return a

    [a, prob] = create_substrings(word_clean, guessed_letters, 4, multiple=True)
    if a != None and prob > pred_prob:
        if verbose:
            print("used4", a)
        return a

    [a, prob] = create_substrings(word_clean, guessed_letters, 3)
    if a != None and prob > pred_prob:
        if verbose:
            print("used3", a)
        return a

    # Fallback: if pred_letter is valid, return it
    if pred_letter and pred_letter not in guessed_letters:
        return pred_letter

    # Fallback: return the most common unguessed letter
    common_letters = "etaoinshrdlcumwfgypbvkjxqz"
    for letter in common_letters:
        if letter not in guessed_letters:
            if verbose:
                print("fallback letter:", letter)
            return letter

    # Final Fallback
    return "a"

class HangmanAPI(object):
    def __init__(self, access_token=None, session=None, timeout=None):
        self.hangman_url = self.determine_hangman_url()
        self.access_token = access_token
        self.session = session or requests.Session()
        self.timeout = timeout
        self.guessed_letters = []

        full_dictionary_location = "words_250000_train.txt"
        self.full_dictionary = self.build_dictionary(full_dictionary_location)
        self.full_dictionary_common_letter_sorted = collections.Counter("".join(self.full_dictionary)).most_common()

        self.current_dictionary = []

    @staticmethod
    def determine_hangman_url():
        links = ['https://trexsim.com', 'https://sg.trexsim.com']

        data = {link: 0 for link in links}

        for link in links:

            requests.get(link)

            for i in range(10):
                s = time.time()
                requests.get(link)
                data[link] = time.time() - s

        link = sorted(data.items(), key=lambda x: x[1])[0][0]
        link += '/trexsim/hangman'
        return link


    def guess(self, word, verbose): # word input example: "_ p p _ e "
        guess_letter = guessing(word = word, guessed_letters = self.guessed_letters,verbose = verbose)
        return guess_letter

    ##########################################################
    # You'll likely not need to modify any of the code below #
    ##########################################################

    def build_dictionary(self, dictionary_file_location):
        text_file = open(dictionary_file_location,"r")
        full_dictionary = text_file.read().splitlines()
        text_file.close()
        return full_dictionary

    def start_game(self, practice=True, verbose=True):
        # reset guessed letters to empty set and current plausible dictionary to the full dictionary
        self.guessed_letters = []
        self.current_dictionary = self.full_dictionary

        response = self.request("/new_game", {"practice":practice})
        if response.get('status')=="approved":
            game_id = response.get('game_id')
            word = response.get('word')
            tries_remains = response.get('tries_remains')
            if verbose:
                print("Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.".format(game_id, tries_remains, word))
            while tries_remains>0:
                # get guessed letter from user code
                guess_letter = self.guess(word, verbose)

                # append guessed letter to guessed letters field in hangman object
                self.guessed_letters.append(guess_letter)
                if verbose:
                    print("Guessing letter: {0}".format(guess_letter))

                try:
                    res = self.request("/guess_letter", {"request":"guess_letter", "game_id":game_id, "letter":guess_letter})
                except HangmanAPIError:
                    print('HangmanAPIError exception caught on request.')
                    continue
                except Exception as e:
                    print('Other exception caught on request.')
                    raise e

                if verbose:
                    print("Sever response: {0}".format(res))
                status = res.get('status')
                tries_remains = res.get('tries_remains')
                if status=="success":
                    if verbose:
                        print("Successfully finished game: {0}".format(game_id))
                    return True
                elif status=="failed":
                    reason = res.get('reason', '# of tries exceeded!')
                    if verbose:
                        print("Failed game: {0}. Because of: {1}".format(game_id, reason))
                    return False
                elif status=="ongoing":
                    word = res.get('word')
        else:
            if verbose:
                print("Failed to start a new game")
        return status=="success"

    def my_status(self):
        return self.request("/my_status", {})

    def request(
            self, path, args=None, post_args=None, method=None):
        if args is None:
            args = dict()
        if post_args is not None:
            method = "POST"

        # Add `access_token` to post_args or args if it has not already been
        # included.
        if self.access_token:
            # If post_args exists, we assume that args either does not exists
            # or it does not need `access_token`.
            if post_args and "access_token" not in post_args:
                post_args["access_token"] = self.access_token
            elif "access_token" not in args:
                args["access_token"] = self.access_token

        time.sleep(0.2)

        num_retry, time_sleep = 50, 2
        for it in range(num_retry):
            try:
                response = self.session.request(
                    method or "GET",
                    self.hangman_url + path,
                    timeout=self.timeout,
                    params=args,
                    data=post_args,
                    verify=False
                )
                break
            except requests.HTTPError as e:
                response = json.loads(e.read())
                raise HangmanAPIError(response)
            except requests.exceptions.SSLError as e:
                if it + 1 == num_retry:
                    raise
                time.sleep(time_sleep)

        headers = response.headers
        if 'json' in headers['content-type']:
            result = response.json()
        elif "access_token" in parse_qs(response.text):
            query_str = parse_qs(response.text)
            if "access_token" in query_str:
                result = {"access_token": query_str["access_token"][0]}
                if "expires" in query_str:
                    result["expires"] = query_str["expires"][0]
            else:
                raise HangmanAPIError(response.json())
        else:
            print("STATUS:", response.status_code)
            print("CONTENT-TYPE:", response.headers.get("Content-Type"))
            print("BODY:", response.text)
            raise HangmanAPIError(f'Maintype was not text, or querystring')

        if result and isinstance(result, dict) and result.get("error"):
            raise HangmanAPIError(result)
        return result

class HangmanAPIError(Exception):
    def __init__(self, result):
        self.result = result
        self.code = None
        try:
            self.type = result["error_code"]
        except (KeyError, TypeError):
            self.type = ""

        try:
            self.message = result["error_description"]
        except (KeyError, TypeError):
            try:
                self.message = result["error"]["message"]
                self.code = result["error"].get("code")
                if not self.type:
                    self.type = result["error"].get("type", "")
            except (KeyError, TypeError):
                try:
                    self.message = result["error_msg"]
                except (KeyError, TypeError):
                    self.message = result

        Exception.__init__(self, self.message)

api = HangmanAPI(access_token="dfaf722dd438b79c4f85a4e73c6114", timeout=2000)

for i in range (100):
  api.start_game(practice=1,verbose=True)
[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)
practice_success_rate = total_practice_successes / total_practice_runs
print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))

for i in range(1000):
    print('Playing ', i, ' th game')
    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission
    api.start_game(practice=0,verbose=False)

    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests
    time.sleep(0.5)

[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)
success_rate = total_recorded_successes/total_recorded_runs
print('overall success rate = %.3f' % success_rate)